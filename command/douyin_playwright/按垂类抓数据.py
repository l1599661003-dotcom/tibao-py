import json
import os
import time
import sys
import configparser

from models.models_tibao import DouyinSearchList
from core.localhost_fp_project import session
from loguru import logger
from playwright.sync_api import sync_playwright
from unitl.common import Common

def get_base_path():
    """è·å–åŸºç¡€è·¯å¾„ï¼Œæ”¯æŒexeæ‰“åŒ…"""
    try:
        return os.path.dirname(os.path.abspath(sys.argv[0])) if hasattr(sys, '_MEIPASS') else os.path.dirname(
            os.path.abspath(__file__))
    except Exception:
        return os.path.abspath("../..")

def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config = configparser.ConfigParser()
    
    # å°è¯•å¤šä¸ªå¯èƒ½çš„é…ç½®æ–‡ä»¶è·¯å¾„
    config_paths = [
        os.path.join(get_base_path(), 'xingtu_spider_config.ini'),
        'xingtu_spider_config.ini'
    ]
    
    config_loaded = False
    for config_path in config_paths:
        if os.path.exists(config_path):
            config.read(config_path, encoding='utf-8')
            config_loaded = True
            logger.info(f"å·²åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
            break
    
    if not config_loaded:
        logger.error("æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ xingtu_spider_config.ini")
        raise FileNotFoundError("é…ç½®æ–‡ä»¶ä¸å­˜åœ¨")
    
    # è§£æé…ç½®
    return {
        'category': {
            'name': config.get('CATEGORY', 'category_name', fallback='æ—¶å°š'),
            'selector': config.get('CATEGORY', 'category_selector', fallback="span:has-text('æ—¶å°š')")
        },
        'price_range': {
            'min_price': config.getint('PRICE_RANGE', 'min_price', fallback=24000),
            'max_price': config.getint('PRICE_RANGE', 'max_price', fallback=30000)
        }
    }

class XingtuSpider:
    """æ˜Ÿå›¾æ•°æ®æŠ“å–å™¨ - ç®€åŒ–ç‰ˆ"""
    
    def __init__(self):
        self.setup_logger()
        # è®¾ç½®loggerå±æ€§
        self.logger = logger
        
        # è®¾ç½®cookieå’Œæ•°æ®ç›®å½•ï¼Œæ”¯æŒexeæ‰“åŒ…
        base_path = get_base_path()
        self.cookie_file = os.path.join(base_path, 'cookies.json')
        self.data_dir = os.path.join(base_path, 'data')
        # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        os.makedirs(self.data_dir, exist_ok=True)
        
        self.base_url = 'https://www.xingtu.cn/ad/creator/market'
        self.is_logged_in = False
        self.api_data = {}  # å­˜å‚¨APIæ•°æ®
        self.common = Common()
        self.current_kol = None  # å½“å‰æ­£åœ¨å¤„ç†çš„KOLä¿¡æ¯
        self.api_response_processed = False  # æ ‡è®°APIå“åº”æ˜¯å¦å·²å¤„ç†
        self.button_clicked = False  # æ ‡è®°æ˜¯å¦å·²ç‚¹å‡»ç¡®å®šæŒ‰é’®
        self.current_page = 1  # å½“å‰é¡µç 
        
        # åŠ è½½é…ç½®
        self.config = load_config()
        logger.info(f"é…ç½®åŠ è½½å®Œæˆ - å“ç±»: {self.config['category']['name']}, ä»·æ ¼åŒºé—´: {self.config['price_range']['min_price']}-{self.config['price_range']['max_price']}")
        
        # æµè§ˆå™¨ç›¸å…³å±æ€§åˆå§‹åŒ–
        self.playwright = None
        self.browser = None
        self.context = None
        self.page = None
        
        # æ•°æ®ç»Ÿè®¡
        self.total_authors = 0
        self.processed_pages = 0

    def setup_logger(self):
        """è®¾ç½®æ—¥å¿—é…ç½®ï¼Œæ”¯æŒexeæ‰“åŒ…"""
        # è®¾ç½®æ—¥å¿—ç›®å½•
        base_path = get_base_path()
        log_path = os.path.join(base_path, 'logs')
        os.makedirs(log_path, exist_ok=True)

        # ç§»é™¤é»˜è®¤å¤„ç†å™¨ï¼Œé¿å…é‡å¤è¾“å‡º
        logger.remove()

        # æ·»åŠ æ§åˆ¶å°è¾“å‡º
        logger.add(
            sys.stdout,
            format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
            level="INFO"
        )

        # æ·»åŠ æ–‡ä»¶è¾“å‡º
        logger.add(
            os.path.join(log_path, "pgy_{time:YYYY-MM-DD}.log"),
            rotation="1 day",
            retention="7 days",
            format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}",
            level="DEBUG",
            encoding="utf-8"
        )
    def setup_browser(self):
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        # å¦‚æœæµè§ˆå™¨å·²ç»åˆå§‹åŒ–ï¼Œç›´æ¥è¿”å›
        if self.browser and self.context and self.page:
            logger.info("æµè§ˆå™¨å·²ç»åˆå§‹åŒ–ï¼Œè·³è¿‡é‡å¤åˆå§‹åŒ–")
            return

        # è®¾ç½®playwrightæµè§ˆå™¨è·¯å¾„ï¼Œæ”¯æŒexeæ‰“åŒ…
        base_path = get_base_path()
        playwright_browsers_path = os.path.join(base_path, 'ms-playwright')

        # è®¾ç½®ç¯å¢ƒå˜é‡
        if os.path.exists(playwright_browsers_path):
            os.environ['PLAYWRIGHT_BROWSERS_PATH'] = playwright_browsers_path
            logger.info(f"ä½¿ç”¨è‡ªå®šä¹‰æµè§ˆå™¨è·¯å¾„: {playwright_browsers_path}")
        else:
            logger.warning(f"æœªæ‰¾åˆ°è‡ªå®šä¹‰æµè§ˆå™¨è·¯å¾„: {playwright_browsers_path}")

        self.playwright = sync_playwright().start()
        # é…ç½®æµè§ˆå™¨é€‰é¡¹
        self.browser = self.playwright.chromium.launch(
            headless=False,
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-gpu',
                '--no-sandbox',
                '--disable-dev-shm-usage',
            ]
        )
        # åˆ›å»ºä¸Šä¸‹æ–‡
        self.context = self.browser.new_context(
            viewport={
                'width': 1512,
                'height': 768
            },
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36'
        )

        # å°è¯•åŠ è½½å·²ä¿å­˜çš„Cookie
        if self._load_cookies():
            # éªŒè¯Cookieæ˜¯å¦æœ‰æ•ˆ
            self.page = self.context.new_page()
            try:
                self.page.goto(self.base_url)
                self.common.random_sleep(1, 2)  # å‡å°‘ç­‰å¾…æ—¶é—´

                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç”¨æˆ·å¤´åƒå…ƒç´ 
                userSide = self.page.locator(".text-avatar").all()
                logger.info(f"æ‰¾åˆ°ç”¨æˆ·å¤´åƒå…ƒç´ æ•°é‡: {len(userSide)}")
                user_len = len(userSide)
                if user_len > 0 or self.page.locator(".text-avatar").is_visible(timeout=3000):  # å‡å°‘è¶…æ—¶æ—¶é—´
                    self.is_logged_in = True
                    logger.info("Cookieæœ‰æ•ˆï¼Œå·²è‡ªåŠ¨ç™»å½•")
                else:
                    logger.info("Cookieå·²å¤±æ•ˆï¼Œéœ€è¦é‡æ–°ç™»å½•")
                    self.is_logged_in = False
            except Exception as e:
                logger.warning(f"CookieéªŒè¯å¤±è´¥: {str(e)}")
                logger.info("å°†è¿›è¡Œé‡æ–°ç™»å½•")
                self.is_logged_in = False
        else:
            self.page = self.context.new_page()
            self.is_logged_in = False

        # è®¾ç½®é¡µé¢è¶…æ—¶æ—¶é—´
        self.page.set_default_timeout(20000)
        # è®¾ç½®å“åº”ç›‘å¬
        self.page.on("response", self._handle_api_response)

        logger.info("æµè§ˆå™¨åˆå§‹åŒ–å®Œæˆ")

    def login(self):
        """
        ç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨ç™»å½•ï¼Œæœ€å¤šç­‰å¾…5åˆ†é’Ÿ
        """
        try:
            if self.is_logged_in:
                logger.info("å·²å¤„äºç™»å½•çŠ¶æ€")
                return True

            try:
                # è®¿é—®é¦–é¡µ
                self.page.goto(self.base_url)
                self.common.random_sleep(1, 2)  # å‡å°‘ç­‰å¾…æ—¶é—´

                # ç­‰å¾…5åˆ†é’Ÿï¼Œæ¯5ç§’æ£€æŸ¥ä¸€æ¬¡ç™»å½•çŠ¶æ€
                max_wait_time = 300  # 5åˆ†é’Ÿ = 300ç§’
                check_interval = 5  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡ï¼Œæé«˜å“åº”é€Ÿåº¦
                elapsed_time = 0

                while elapsed_time < max_wait_time:
                    try:
                        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç”¨æˆ·å¤´åƒå…ƒç´ ï¼ˆç™»å½•æˆåŠŸçš„æ ‡å¿—ï¼‰
                        user_avatar = self.page.locator(".text-avatar").first
                        if user_avatar and user_avatar.is_visible():
                            logger.info("æ£€æµ‹åˆ°ç™»å½•æˆåŠŸï¼")
                            self.is_logged_in = True

                            # ç™»å½•æˆåŠŸåä¿å­˜Cookie
                            self._save_cookies()

                            return True
                        time.sleep(check_interval)
                        elapsed_time += check_interval

                    except Exception as e:
                        logger.warning(f"æ£€æŸ¥ç™»å½•çŠ¶æ€æ—¶å‡ºé”™: {str(e)}")
                        time.sleep(check_interval)
                        elapsed_time += check_interval

                # 5åˆ†é’Ÿè¶…æ—¶ï¼Œä»æœªç™»å½•æˆåŠŸ
                logger.error("ç­‰å¾…ç™»å½•è¶…æ—¶ï¼ˆ5åˆ†é’Ÿï¼‰ï¼Œç¨‹åºé€€å‡º")
                return False

            except Exception as e:
                logger.error(f"ç­‰å¾…ç™»å½•è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {str(e)}")
                return False

        except Exception as e:
            logger.error(f"ç™»å½•è¿‡ç¨‹å‡ºç°å¼‚å¸¸: {str(e)}")
            return False

    def scrape_data(self):
        """æŠ“å–æ•°æ®ä¸»æµç¨‹"""
        try:
            if not self.is_logged_in:
                logger.error("æœªç™»å½•çŠ¶æ€")
                return False
            
            min_price = self.config['price_range']['min_price']
            max_price = self.config['price_range']['max_price']
            category = self.config['category']['name']
            
            logger.info(f"å¼€å§‹æŠ“å–æ•°æ® - å“ç±»: {category}, ä»·æ ¼åŒºé—´: {min_price}-{max_price}")
            
            # è®¾ç½®ç­›é€‰æ¡ä»¶
            self._set_filters(min_price, max_price)
            
            # å¤„ç†æ‰€æœ‰é¡µé¢
            self._process_all_pages()
            
            logger.info(f"ğŸ‰ æŠ“å–å®Œæˆï¼å…±å¤„ç† {self.processed_pages} é¡µï¼Œç´¯è®¡æ–°å¢ {self.total_authors} æ¡æ•°æ®")
            return True
            
        except Exception as e:
            logger.error(f"æŠ“å–æ•°æ®å¤±è´¥: {e}")
            return False

    def _set_filters(self, min_price, max_price):
        """è®¾ç½®ç­›é€‰æ¡ä»¶"""
        try:
            category = self.config['category']['name']
            
            # ç‚¹å‡»å“ç±»æ ‡ç­¾
            self.page.locator(self.config['category']['selector']).first.click()
            time.sleep(1)
            
            # ç‚¹å‡»å…¨é€‰
            self.page.locator("span:has-text('å…¨é€‰')").first.click()
            time.sleep(2)

            # å†æ¬¡ç‚¹å‡»å“ç±»æ ‡ç­¾
            self.page.locator(self.config['category']['selector']).first.click()
            time.sleep(1)
            
            # ç‚¹å‡»è¾¾äººæŠ¥ä»·
            self.page.locator("span:has-text('è¾¾äººæŠ¥ä»·')").first.click()
            time.sleep(2)
            
            # ç‚¹å‡»æŠ¥ä»·åŒºé—´ä¸‹æ‹‰æ¡†
            price_item = self.page.locator("div.price-group-item:has(div.label:has-text('æŠ¥ä»·åŒºé—´'))").first
            price_item.locator("div.xt-dropdown").first.click()
            time.sleep(1)
            
            # è®¾ç½®ä»·æ ¼åŒºé—´
            inputs = self.page.locator("div.input-wrapper.el-input input[type='number']")
            inputs.nth(0).fill(str(min_price))
            inputs.nth(1).fill(str(max_price))
            time.sleep(1)
            
            # ç‚¹å‡»ç¬¬ä¸€ä¸ªç¡®å®šæŒ‰é’®
            self.page.locator("div.custom-actions button.submit-btn:has-text('ç¡®å®š')").first.click()
            time.sleep(2)
            
            # æ¸…ç©ºAPIæ•°æ®ç¼“å­˜ï¼ˆå…³é”®ï¼ï¼‰
            self.api_data = {}
            # é‡ç½®æŒ‰é’®ç‚¹å‡»çŠ¶æ€
            self.button_clicked = False
            
            # ç‚¹å‡»ç¬¬äºŒä¸ªç¡®å®šæŒ‰é’®
            self.page.locator("footer button.el-button--primary:has-text('ç¡®å®š')").first.click()
            
            # è®¾ç½®æŒ‰é’®ç‚¹å‡»æ ‡å¿—
            self.button_clicked = True
            logger.info("å·²è®¾ç½®æŒ‰é’®ç‚¹å‡»æ ‡å¿—ï¼Œåç»­APIå“åº”å°†è¢«å¤„ç†")
            
            try:
                self.page.wait_for_load_state('networkidle', timeout=3000)  # å‡å°‘è¶…æ—¶æ—¶é—´
            except Exception as e:
                logger.warning(f"ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆæ—¶å‡ºé”™: {str(e)}")
            
            # ç­‰å¾…APIæ•°æ®åŠ è½½
            time.sleep(2)  # å‡å°‘ç­‰å¾…æ—¶é—´
            
            logger.info(f"ç­›é€‰æ¡ä»¶è®¾ç½®å®Œæˆ - å“ç±»: {category}, ä»·æ ¼: {min_price}-{max_price}")
            
        except Exception as e:
            logger.error(f"è®¾ç½®ç­›é€‰æ¡ä»¶å¤±è´¥: {e}")
            raise

    def _process_all_pages(self):
        """å¤„ç†æ‰€æœ‰é¡µé¢"""
        page_num = 1
        
        # å¤„ç†ç¬¬ä¸€é¡µï¼ˆç‚¹å‡»ç¡®å®šåçš„å½“å‰é¡µï¼‰
        logger.info(f"å¼€å§‹å¤„ç†ç¬¬ {page_num} é¡µæ•°æ®...")
        self._process_current_page_data(page_num)
        
        # è‡ªåŠ¨ç¿»é¡µå¤„ç†åç»­é¡µé¢
        while self._click_next_page():
            page_num += 1
            self.processed_pages = page_num
            logger.info(f"æ­£åœ¨å¤„ç†ç¬¬ {page_num} é¡µæ•°æ®...")

            
            # ç­‰å¾…é¡µé¢ç½‘ç»œç©ºé—²
            try:
                self.page.wait_for_load_state('networkidle', timeout=3000)  # å‡å°‘è¶…æ—¶æ—¶é—´
            except Exception as e:
                logger.warning(f"ç­‰å¾…ç½‘ç»œç©ºé—²æ—¶å‡ºé”™: {str(e)}")
            
            # å¤„ç†å½“å‰é¡µæ•°æ®
            self._process_current_page_data(page_num)
            
            # å¦‚æœä¸æ˜¯æœ€åä¸€é¡µï¼Œç­‰å¾…ä¸€ä¸‹å†å¤„ç†ä¸‹ä¸€é¡µ
            time.sleep(1)  # å‡å°‘ç­‰å¾…æ—¶é—´
        
        logger.info(f"æ‰€æœ‰é¡µé¢å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {self.processed_pages} é¡µ")

    def _process_current_page_data(self, page_num):
        """å¤„ç†å½“å‰é¡µé¢çš„æ•°æ®"""
        try:
            if not self.api_data:
                logger.warning(f"ç¬¬ {page_num} é¡µæ²¡æœ‰APIæ•°æ®")
                return 0

            logger.info(f"å¤„ç†ç¬¬ {page_num} é¡µçš„ {len(self.api_data)} ä¸ªAPIå“åº”")

            # å¤„ç†APIæ•°æ®å¹¶ä¿å­˜åˆ°æ•°æ®åº“
            api_data_copy = dict(self.api_data)
            total_authors_added = 0
            total_authors_skipped = 0

            for api_url, response_data in api_data_copy.items():
                if 'data' not in response_data:
                    continue

                api_data = response_data['data']
                if 'q' in api_url:
                    authors_added = 0
                    authors_skipped = 0

                    # ç¡®ä¿authorså­—æ®µå­˜åœ¨
                    if 'authors' not in api_data:
                        logger.warning(f"APIæ•°æ®ç¼ºå°‘authorså­—æ®µ: {list(api_data.keys()) if isinstance(api_data, dict) else 'Not a dict'}")
                        continue

                    for author_data in api_data['authors']:
                        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                        existing_data = session.query(DouyinSearchList).filter(
                            DouyinSearchList.star_id == author_data['star_id']).first()
                        if existing_data:
                            authors_skipped += 1
                            continue

                        # åˆ›å»ºæ–°è®°å½•
                        detail = DouyinSearchList(
                            attribute_datas=json.dumps(author_data.get('attribute_datas', {}), ensure_ascii=False),
                            extra_data=json.dumps(author_data.get('extra_data', {}), ensure_ascii=False),
                            items=json.dumps(author_data.get('items', []), ensure_ascii=False),
                            star_id=str(author_data.get('star_id', '')),
                            task_infos=json.dumps(author_data.get('task_infos', {}), ensure_ascii=False),
                            category=self.config['category']['name'],
                        )
                        session.add(detail)
                        authors_added += 1

                    # ç´¯è®¡æ€»æ•°
                    total_authors_added += authors_added
                    total_authors_skipped += authors_skipped

                    # æ‰¹é‡æäº¤äº‹åŠ¡ï¼Œæé«˜æ•ˆç‡
                    try:
                        session.commit()
                        logger.info(f"APIå“åº”å¤„ç†å®Œæˆ: æ–°å¢ {authors_added} æ¡ï¼Œè·³è¿‡ {authors_skipped} æ¡é‡å¤æ•°æ®")
                    except Exception as commit_error:
                        logger.error(f"æäº¤äº‹åŠ¡æ—¶å‡ºé”™: {str(commit_error)}")
                        session.rollback()

            # æ¸…ç©ºå·²å¤„ç†çš„APIæ•°æ®ï¼Œé¿å…é‡å¤å¤„ç†
            for url in api_data_copy.keys():
                if url in self.api_data:
                    del self.api_data[url]

            logger.info(f"ç¬¬ {page_num} é¡µæ•°æ®å¤„ç†å®Œæˆ: æ€»è®¡æ–°å¢ {total_authors_added} æ¡ï¼Œè·³è¿‡ {total_authors_skipped} æ¡é‡å¤æ•°æ®")
            return total_authors_added

        except Exception as e:
            logger.error(f"å¤„ç†ç¬¬ {page_num} é¡µæ•°æ®æ—¶å‡ºé”™: {str(e)}")
            session.rollback()  # å‡ºé”™æ—¶å›æ»šäº‹åŠ¡
            return 0

    def _click_next_page(self):
        """ç‚¹å‡»ä¸‹ä¸€é¡µæŒ‰é’®ï¼Œè¿”å›æ˜¯å¦æˆåŠŸç‚¹å‡»"""
        try:
            # æŸ¥æ‰¾ä¸‹ä¸€é¡µæŒ‰é’®ï¼ˆæ ¹æ®æ‚¨ä¹‹å‰æä¾›çš„HTMLç»“æ„ï¼‰
            next_page_button = self.page.locator("button.btn-next").first

            # æ£€æŸ¥æŒ‰é’®æ˜¯å¦å­˜åœ¨
            if not next_page_button.is_visible(timeout=2000):  # å‡å°‘è¶…æ—¶æ—¶é—´
                logger.info("æœªæ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®ï¼Œå¯èƒ½å·²åˆ°æœ€åä¸€é¡µ")
                return False

            # æ£€æŸ¥æŒ‰é’®æ˜¯å¦è¢«ç¦ç”¨
            button_disabled = next_page_button.get_attribute("disabled")
            if button_disabled is not None:
                logger.info("ä¸‹ä¸€é¡µæŒ‰é’®è¢«ç¦ç”¨ï¼Œç¡®è®¤å·²åˆ°æœ€åä¸€é¡µ")
                return False

            # ç‚¹å‡»ä¸‹ä¸€é¡µæŒ‰é’®
            logger.info("ç‚¹å‡»ä¸‹ä¸€é¡µæŒ‰é’®")
            next_page_button.click()

            # ç­‰å¾…ç½‘ç»œç©ºé—²
            try:
                self.page.wait_for_load_state('networkidle', timeout=3000)  # å‡å°‘è¶…æ—¶æ—¶é—´
            except Exception as e:
                logger.warning(f"ç­‰å¾…ç½‘ç»œç©ºé—²æ—¶å‡ºé”™: {str(e)}")

            return True

        except Exception as e:
            logger.error(f"ç‚¹å‡»ä¸‹ä¸€é¡µæ—¶å‡ºé”™: {str(e)}")
            return False

    def _handle_api_response(self, response):
        """å¤„ç†APIå“åº”"""
        try:
            url = response.url
            # è®¾ç½®æ›´å¹¿æ³›çš„ç›®æ ‡APIå…³é”®è¯
            target_apis = ['api/gsearch/search_for_author_square']

            # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®æ ‡API
            is_target_api = any(api in url for api in target_apis)

            if is_target_api and (response.request.resource_type == 'fetch' or response.request.resource_type == 'xhr'):
                logger.info(f"æ•è·åˆ°ç›®æ ‡API: {url}")
                try:
                    # æ£€æŸ¥å“åº”çŠ¶æ€
                    if response.status != 200:
                        logger.warning(f"APIå“åº”çŠ¶æ€å¼‚å¸¸: {response.status}, URL: {url}")
                        return

                    try:
                        # æ£€æŸ¥æµè§ˆå™¨æ˜¯å¦ä»ç„¶æœ‰æ•ˆ
                        if not hasattr(self, 'page') or not self.page or self.page.is_closed():
                            logger.warning(f"é¡µé¢å·²å…³é—­ï¼Œè·³è¿‡APIæ•°æ®å¤„ç†: {url}")
                            return

                        data = response.json()

                        # æ‰¾åˆ°åŒ¹é…çš„APIç±»å‹
                        matched_api = None
                        for api in target_apis:
                            if api in url:
                                matched_api = api
                                break

                        # å­˜å‚¨æœ‰æ•ˆçš„APIæ•°æ®
                        if matched_api:
                            # æ£€æŸ¥å½“å‰æ˜¯å¦å¤„äºç‚¹å‡»æŒ‰é’®åçš„çŠ¶æ€
                            if hasattr(self, 'button_clicked') and self.button_clicked:
                                self.api_data[url] = {
                                    'url': url,
                                    'data': data,
                                    'api_type': matched_api,
                                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                                    'status': response.status
                                }
                                logger.info(f"æˆåŠŸå­˜å‚¨APIæ•°æ®: {url}, å½“å‰APIæ•°æ®æ€»æ•°: {len(self.api_data)}")

                                # æ£€æŸ¥æ•°æ®ç»“æ„
                                if 'authors' in data:
                                    authors_count = len(data['authors'])
                                    logger.info(f"APIæ•°æ®åŒ…å« {authors_count} ä¸ªä½œè€…ä¿¡æ¯")

                                    # ç«‹å³å¤„ç†æ•°æ®å¹¶ä¿å­˜åˆ°æ•°æ®åº“
                                    self._process_api_data_immediately(url, data)
                                else:
                                    logger.warning(f"APIæ•°æ®ç»“æ„å¼‚å¸¸: {list(data.keys()) if isinstance(data, dict) else 'Not a dict'}")

                            # æ ‡è®°APIå“åº”å·²å¤„ç†
                            self.api_response_processed = True
                        else:
                            logger.info(f"æ•è·åˆ°APIæ•°æ®ï¼Œä½†å°šæœªç‚¹å‡»æŒ‰é’®ï¼Œæš‚ä¸å¤„ç†: {url}")

                    except ValueError:
                        logger.warning(f"æ— æ•ˆçš„JSONå“åº”: {url}")
                    except Exception as json_error:
                        logger.warning(f"JSONè§£æå¤±è´¥: {str(json_error)}, URL: {url}")

                except Exception as e:
                    # å¦‚æœæ˜¯æµè§ˆå™¨å…³é—­é”™è¯¯ï¼Œä¸è®°å½•ä¸ºé”™è¯¯
                    if "Target page, context or browser has been closed" in str(e):
                        logger.info(f"æµè§ˆå™¨å·²å…³é—­ï¼Œè·³è¿‡APIæ•°æ®å¤„ç†: {url}")
                    else:
                        logger.error(f"å¤„ç†APIæ•°æ®æ—¶å‡ºé”™: {str(e)}, URL: {url}")
        except Exception as e:
            logger.error(f"å¤„ç†APIå“åº”æ—¶å‡ºé”™: {str(e)}")

    def _process_api_data_immediately(self, url, data):
        """ç«‹å³å¤„ç†APIæ•°æ®ï¼Œé¿å…å»¶è¿Ÿå¯¼è‡´çš„æ•°æ®ä¸¢å¤±"""
        try:
            if 'search_for_author_square' in url:
                if data and 'authors' in data:
                    authors_added = 0
                    authors_skipped = 0

                    for author_data in data['authors']:
                        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                        existing_data = session.query(DouyinSearchList).filter(
                            DouyinSearchList.star_id == author_data['star_id']).first()
                        if existing_data:
                            authors_skipped += 1
                            continue

                        # åˆ›å»ºæ–°è®°å½•
                        detail = DouyinSearchList(
                            attribute_datas=json.dumps(author_data.get('attribute_datas', {}), ensure_ascii=False),
                            extra_data=json.dumps(author_data.get('extra_data', {}), ensure_ascii=False),
                            items=json.dumps(author_data.get('items', []), ensure_ascii=False),
                            star_id=str(author_data.get('star_id', '')),
                            task_infos=json.dumps(author_data.get('task_infos', {}), ensure_ascii=False),
                            category=self.config['category']['name'],
                        )
                        session.add(detail)
                        authors_added += 1

                    # æäº¤æ•°æ®åº“äº‹åŠ¡
                    session.commit()
                    self.total_authors += authors_added
                    logger.info(f"ç«‹å³å¤„ç†APIæ•°æ®å®Œæˆ: æ–°å¢ {authors_added} æ¡ï¼Œè·³è¿‡ {authors_skipped} æ¡é‡å¤æ•°æ®")
                else:
                    logger.warning(f"APIæ•°æ®ç»“æ„å¼‚å¸¸ï¼Œæ— æ³•å¤„ç†: {url}")

        except Exception as e:
            logger.error(f"ç«‹å³å¤„ç†APIæ•°æ®æ—¶å‡ºé”™: {str(e)}")
            session.rollback()

    def _save_author_data(self, author_data):
        """ä¿å­˜ä½œè€…æ•°æ®"""
        try:
            star_id = str(author_data.get('star_id', ''))
            nick_name = ""
            
            # å°è¯•è·å–æ˜µç§°ç”¨äºæ—¥å¿—æ˜¾ç¤º
            try:
                attribute_datas = author_data.get('attribute_datas', {})
                if isinstance(attribute_datas, dict):
                    nick_name = attribute_datas.get('nick_name', '')
                elif isinstance(attribute_datas, str):
                    import json
                    attr_data = json.loads(attribute_datas)
                    nick_name = attr_data.get('nick_name', '')
            except:
                nick_name = "æœªçŸ¥"
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing = session.query(DouyinSearchList).filter(
                DouyinSearchList.star_id == star_id
            ).first()
            
            if existing:
                logger.info(f"ğŸ“‹ æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡: {nick_name} (ID: {star_id})")
                return False
                
            # åˆ›å»ºæ–°è®°å½•
            new_record = DouyinSearchList(
                star_id=star_id,
                attribute_datas=json.dumps(author_data.get('attribute_datas', {}), ensure_ascii=False),
                extra_data=json.dumps(author_data.get('extra_data', {}), ensure_ascii=False),
                items=json.dumps(author_data.get('items', []), ensure_ascii=False),
                task_infos=json.dumps(author_data.get('task_infos', {}), ensure_ascii=False),
                category=self.config['category']['name'],
            )
            
            session.add(new_record)
            logger.info(f"âœ… æ–°å¢æ•°æ®: {nick_name} (ID: {star_id}) - å“ç±»: {self.config['category']['name']}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ•°æ®å¤±è´¥: {e}")
            session.rollback()
            return False

    def _save_cookies(self):
        """
        ä¿å­˜å½“å‰ä¼šè¯çš„Cookieåˆ°åŒçº§ç›®å½•
        """
        try:
            cookies = self.context.cookies()
            # ç¡®ä¿cookieæ–‡ä»¶çš„ç›®å½•å­˜åœ¨
            cookie_dir = os.path.dirname(self.cookie_file)
            if cookie_dir and not os.path.exists(cookie_dir):
                os.makedirs(cookie_dir, exist_ok=True)

            with open(self.cookie_file, 'w', encoding='utf-8') as f:
                json.dump(cookies, f, indent=2, ensure_ascii=False)
            logger.info(f"Cookieå·²ä¿å­˜åˆ°: {self.cookie_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜Cookieæ—¶å‡ºé”™: {str(e)}")

    def _load_cookies(self):
        """
        ä»åŒçº§ç›®å½•åŠ è½½ä¿å­˜çš„Cookie
        :return: æ˜¯å¦æˆåŠŸåŠ è½½Cookie
        """
        try:
            if os.path.exists(self.cookie_file):
                with open(self.cookie_file, 'r', encoding='utf-8') as f:
                    cookies = json.load(f)

                if cookies:
                    self.context.add_cookies(cookies)
                    logger.info(f"å·²æˆåŠŸåŠ è½½ {len(cookies)} ä¸ªCookie")
                    return True
                else:
                    logger.warning("Cookieæ–‡ä»¶ä¸ºç©º")
                    return False
            else:
                return False
        except Exception as e:
            logger.error(f"åŠ è½½Cookieæ—¶å‡ºé”™: {str(e)}")
            # å¦‚æœcookieæ–‡ä»¶æŸåï¼Œåˆ é™¤å®ƒ
            try:
                if os.path.exists(self.cookie_file):
                    os.remove(self.cookie_file)
                    logger.info("å·²åˆ é™¤æŸåçš„Cookieæ–‡ä»¶")
            except:
                pass
            return False

    def close(self):
        """
        å…³é—­æµè§ˆå™¨ã€playwrightå’Œæ•°æ®åº“è¿æ¥
        """
        try:
            # ä¿å­˜Cookie
            if self.is_logged_in:
                self._save_cookies()

            # æ£€æŸ¥æµè§ˆå™¨æ˜¯å¦å·²åˆå§‹åŒ–
            if hasattr(self, 'page') and self.page:
                self.page.close()
            if hasattr(self, 'context') and self.context:
                self.context.close()
            if hasattr(self, 'browser') and self.browser:
                self.browser.close()
            if hasattr(self, 'playwright') and self.playwright:
                self.playwright.stop()

            logger.info("æµè§ˆå™¨å’Œplaywrightå·²å…³é—­")
        except Exception as e:
            logger.error(f"å…³é—­èµ„æºæ—¶å‡ºé”™: {str(e)}")


def main():
    """
    ä¸»å‡½æ•° - æ˜Ÿå›¾æ•°æ®æŠ“å–ç¨‹åº
    """
    spider = None
    try:
        logger.info("=== æ˜Ÿå›¾æ•°æ®æŠ“å–ç¨‹åºå¯åŠ¨ ===")
        spider = XingtuSpider()

        # 3. åˆå§‹åŒ–æµè§ˆå™¨å’Œç™»å½•
        spider.setup_browser()
        login_success = spider.login()
        if not login_success:
            logger.error("ç™»å½•å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
            return False

        # 4. æŠ“å–æ•°æ®
        result = spider.scrape_data()
        if not result:
            logger.error("æŠ“å–æ•°æ®å¤±è´¥")
            return False

        logger.info("æ˜Ÿå›¾æ•°æ®æŠ“å–ç¨‹åºæ‰§è¡Œå®Œæˆ")
        return True

    except KeyboardInterrupt:
        logger.warning("ç”¨æˆ·æ‰‹åŠ¨ä¸­æ–­ç¨‹åº")
        return False
    except Exception as e:
        logger.error(f"ç¨‹åºè¿è¡Œå‡ºé”™: {str(e)}")
        import traceback
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        return False
    finally:
        # ç¡®ä¿èµ„æºè¢«æ­£ç¡®é‡Šæ”¾
        if spider:
            try:
                spider.close()
                logger.info("èµ„æºæ¸…ç†å®Œæˆ")
            except Exception as e:
                logger.error(f"æ¸…ç†èµ„æºæ—¶å‡ºé”™: {str(e)}")
        session.close()


if __name__ == "__main__":
    try:
        success = main()
        if success:
            logger.info("ç¨‹åºæ‰§è¡ŒæˆåŠŸ")
            sys.exit(0)
        else:
            logger.error("ç¨‹åºæ‰§è¡Œå¤±è´¥")
            sys.exit(1)
    except Exception as e:
        logger.error(f"ç¨‹åºå¯åŠ¨å¤±è´¥: {str(e)}")
        sys.exit(1)
