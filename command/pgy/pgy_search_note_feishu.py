import requests
import json
import time
import urllib3
import pandas as pd
from datetime import datetime
import os


class GetPGYMediaAll:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
            "cookie": 'a1=19a5eb1a425pmd1q4v02v1zlqfls7z25o8f34547h50000131391; webId=58e94014b58a170000190f4b2283aaaf; customer-sso-sid=68c517569988680951332869hzm0ncku1hk7oudo; x-user-id-pgy.xiaohongshu.com=68fb0f1c1558000000000001; customerClientId=064982114373578; xsecappid=ratlin; solar.beaker.session.id=AT-68c5175725519646781931555rzlmzneowaiiowl; access-token-pgy.xiaohongshu.com=customer.pgy.AT-68c5175725519646781931555rzlmzneowaiiowl; access-token-pgy.beta.xiaohongshu.com=customer.pgy.AT-68c5175725519646781931555rzlmzneowaiiowl; acw_tc=0a42252317631247772785371e8c7a86e54ceaa6cdc9f66bf51b34b4e33f62; loadts=1763125630940',
        }

    def _extract_tags(self, blogger):
        """æå–å¹¶æ‹¼æ¥æ ‡ç­¾"""
        tags_list = []
        
        # å¤„ç† contentTags
        content_tags = blogger.get('contentTags', [])
        if content_tags and isinstance(content_tags, list):
            for content_tag in content_tags:
                if not isinstance(content_tag, dict):
                    continue
                # æ·»åŠ  taxonomy1Tag
                taxonomy1_tag = content_tag.get('taxonomy1Tag', '')
                if taxonomy1_tag:
                    tags_list.append(taxonomy1_tag)
                # å¾ªç¯æ·»åŠ  taxonomy2Tags
                taxonomy2_tags = content_tag.get('taxonomy2Tags', [])
                if taxonomy2_tags and isinstance(taxonomy2_tags, list):
                    tags_list.extend([tag for tag in taxonomy2_tags if tag])
        
        # å¤„ç† featureTags
        feature_tags = blogger.get('featureTags', [])
        if feature_tags and isinstance(feature_tags, list):
            tags_list.extend([tag for tag in feature_tags if tag])
        
        # æ‹¼æ¥æ‰€æœ‰æ ‡ç­¾ï¼Œç”¨ä¸­æ–‡é€—å·åˆ†éš”
        return 'ã€'.join(tags_list) if tags_list else ''

    def handle(self):
        base_data = {"searchType":0,"column":"comprehensiverank","sort":"desc","pageSize":20,"brandUserId":"62b58a79000000001b024664","marketTarget":None,"audienceGroup":[],"contentTag":["å®¶å±…è£…é¥°","èŠ±è‰ºå›­è‰º","å®¶å±…ç”¨å“","è£…ä¿®","å®¶å…·","å®¤å†…è®¾è®¡","å®¶å±…å®¶è£…å…¶ä»–","å±…å®¶ç»éªŒ"],"personalTags":[],"gender":None,"location":None,"signed":-1,"featureTags":[],"fansNumberLower":50000,"fansNumberUpper":None,"fansAge":0,"fansGender":0,"accumCommonImpMedinNum30d":[],"readMidNor30":[],"interMidNor30":[],"thousandLikePercent30":[],"noteType":0,"videoPriceLower":3000,"videoPriceUpper":8500,"progressOrderCnt":[],"tradeType":"ä¸é™","tradeReportBrandIdSet":[],"excludedTradeReportBrandId":False,"estimateCpuv30d":[],"inStar":0,"firstIndustry":"","secondIndustry":"","newHighQuality":0,"filterIntention":False,"flagList":[{"flagType":"HAS_BRAND_COOP_BUYER_AUTH","flagValue":"0"},{"flagType":"IS_HIGH_QUALITY","flagValue":"0"}],"activityCodes":[],"excludeLowActive":True,"fansNumUp":0,"excludedTradeReportBrand":False,"excludedTradeInviteReportBrand":False,"filterList":[]}

        all_data = []  # å­˜å‚¨æ‰€æœ‰åšä¸»æ•°æ®

        for i in range(1, 200):
            data = base_data.copy()
            data['pageNum'] = i

            try:
                # First request to get trackId
                response = requests.post(
                    "https://pgy.xiaohongshu.com/api/solar/cooperator/blogger/track",
                    headers=self.headers,
                    json=data,
                    verify=False
                )
                response_data = response.json()

                data['trackId'] = response_data['data']['trackId']

                # Sleep to avoid rate limiting
                time.sleep(6)

                # Second request to get blogger data
                response1 = requests.post(
                    "https://pgy.xiaohongshu.com/api/solar/cooperator/blogger/v2",
                    headers=self.headers,
                    json=data,
                    verify=False
                )

                bloggers = response1.json()['data']['kols']

                if len(bloggers) == 0:
                    print(f"ç¬¬ {i} é¡µæ²¡æœ‰æ•°æ®ï¼Œå·²åˆ°è¾¾æœ€åä¸€é¡µ")
                    break

                # æ”¶é›†è¿™ä¸€é¡µçš„æ‰€æœ‰åšä¸»æ•°æ®
                for blogger in bloggers:
                    # æå–æ ‡ç­¾
                    tags = self._extract_tags(blogger)
                    
                    # æ„å»ºæ•°æ®è¡Œ
                    row_data = {
                        'è’²å…¬è‹±é“¾æ¥': f"https://pgy.xiaohongshu.com/solar/pre-trade/blogger-detail/{blogger.get('userId', '')}",
                        'åšä¸»èº«ä»½æ ‡ç­¾': 'ã€'.join(blogger.get('personalTags', [])) if isinstance(blogger.get('personalTags', []), list) else blogger.get('personalTags', ''),
                        'æ˜µç§°': blogger.get('name', ''),
                        'ä¸»é¡µé“¾æ¥': f"https://www.xiaohongshu.com/user/profile/{blogger.get('userId', '')}",
                        'é˜…è¯»ä¸­ä½æ•°ï¼ˆæ—¥å¸¸ï¼‰': blogger.get('clickMidNum', ''),
                        'äº’åŠ¨ä¸­ä½æ•°ï¼ˆæ—¥å¸¸ï¼‰': blogger.get('mengagementNum', ''),
                        'æ€§åˆ«': blogger.get('gender', ''),
                        'åœ°åŒº': blogger.get('location', ''),
                        'å›¾æ–‡æŠ¥ä»·': blogger.get('picturePrice', ''),
                        'è§†é¢‘æŠ¥ä»·': blogger.get('videoPrice', ''),
                        'åˆä½œè¡Œä¸š': blogger.get('tradeType', ''),
                        'å°çº¢ä¹¦ID': blogger.get('redId', ''),
                        'ç²‰ä¸æ•°': blogger.get('fansNum', ''),
                        'æ ‡ç­¾': tags,
                    }
                    all_data.append(row_data)

                print(f"å®Œæˆç¬¬ {i} é¡µï¼Œå…±æ”¶é›† {len(all_data)} æ¡æ•°æ®")
                time.sleep(2)  # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«

            except requests.RequestException as e:
                print(f"ç¬¬ {i} é¡µè¯·æ±‚é”™è¯¯: {str(e)}")
                continue
            except Exception as e:
                print(f"ç¬¬ {i} é¡µå‘ç”Ÿæ„å¤–é”™è¯¯: {str(e)}")
                continue

        # å¯¼å‡ºæ•°æ®åˆ°Excel
        if all_data:
            self._export_to_excel(all_data)
        else:
            print("æ²¡æœ‰æ”¶é›†åˆ°ä»»ä½•æ•°æ®")

    def _export_to_excel(self, data):
        """å¯¼å‡ºæ•°æ®åˆ°Excelæ–‡ä»¶"""
        try:
            # åˆ›å»ºDataFrame
            df = pd.DataFrame(data)
            
            # ç”Ÿæˆæ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'è’²å…¬è‹±åšä¸»æ•°æ®_{timestamp}.xlsx'
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_dir = 'output'
            if not os.path.exists(output_dir):
                os.makedirs(output_dir)
            
            filepath = os.path.join(output_dir, filename)
            
            # å¯¼å‡ºåˆ°Excel
            df.to_excel(filepath, index=False, engine='openpyxl')
            
            print(f"\nâœ… æ•°æ®å¯¼å‡ºæˆåŠŸï¼")
            print(f"ğŸ“ æ–‡ä»¶è·¯å¾„: {filepath}")
            print(f"ğŸ“Š å…±å¯¼å‡º {len(data)} æ¡æ•°æ®")
            
        except Exception as e:
            print(f"âŒ å¯¼å‡ºExcelå¤±è´¥: {str(e)}")
            # å¦‚æœå¯¼å‡ºå¤±è´¥ï¼Œå°è¯•ä¿å­˜åˆ°å½“å‰ç›®å½•
            try:
                fallback_filename = f'è’²å…¬è‹±åšä¸»æ•°æ®_{timestamp}.xlsx'
                df.to_excel(fallback_filename, index=False, engine='openpyxl')
                print(f"âœ… å·²ä¿å­˜åˆ°å½“å‰ç›®å½•: {fallback_filename}")
            except Exception as e2:
                print(f"âŒ å¤‡ç”¨å¯¼å‡ºä¹Ÿå¤±è´¥: {str(e2)}")


if __name__ == "__main__":
    # Disable SSL warnings
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

    # Run the script
    script = GetPGYMediaAll()
    script.handle()
